model_name      : "meta-llama/Llama-2-7b-hf"
dataset_name    : "lvwerra/stack-exchange-paired")
subset          : "data/finetune"
split           : "train"
size_valid_set  : 4000
streaming       : True
shuffle_buffer  : 5000
seq_length      : 1024
num_workers     : 4
use_bnb         : True


training_args : {
    "output_dir"                  : "./sft_model_output", 
    "per_device_train_batch_size" : 4,  
    "per_device_eval_batch_size"  : 4,   
    "learning_rate"               : 2e-5,             
    "num_train_epochs"            : 1,             
    "max_seq_length"              : 1024, 
    "dataset_text_field"          : "text", 
    "gradient_accumulation_steps" : 4, 
    "bf16"                        : True, # Use bfloat16 if your GPU supports it (Ampere architecture or newer)
    "fp16"                        : False, # Otherwise, use float16. Don't use both.
    "gradient_checkpointing"      : True, 
    "lora_r"                      : 8,          # Rank of the LoRA matrices
    "lora_alpha"                  : 16,     # Scaling factor
    "lora_dropout"                : 0.05, # Dropout for LoRA layers
    "lora_target_modules"         : ["q_proj", "v_proj"], # Which modules to apply LoRA to.
                                               
}

